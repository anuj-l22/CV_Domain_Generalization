{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8eae149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# For SNIP we need functional for loss computation\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Path to the PACS dataset root directory (adjust this as necessary)\n",
    "DATA_DIR = \"/home/rishabh/Anuj_Sem6/CV_S6/DomainBed/domainbed/data\"  # Update with your actual dataset directory\n",
    "\n",
    "# Import DomainBed utilities (ensure DomainBed is in your Python path)\n",
    "from domainbed import algorithms, datasets, hparams_registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0b6953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_algorithm_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a DomainBed model from model_path.\n",
    "    Returns the instantiated algorithm and dump information.\n",
    "    \"\"\"\n",
    "    dump = torch.load(model_path, map_location=device)\n",
    "    algo_name = dump[\"args\"][\"algorithm\"]\n",
    "    AlgorithmClass = algorithms.get_algorithm_class(algo_name)\n",
    "    algorithm = AlgorithmClass(\n",
    "        dump[\"model_input_shape\"],\n",
    "        dump[\"model_num_classes\"],\n",
    "        dump[\"model_num_domains\"],\n",
    "        dump[\"model_hparams\"]\n",
    "    )\n",
    "    algorithm.load_state_dict(dump[\"model_dict\"])\n",
    "    algorithm.to(device)\n",
    "    algorithm.eval()\n",
    "    return algorithm, dump\n",
    "\n",
    "def evaluate_model_on_pacs(algorithm, target_env):\n",
    "    \"\"\"\n",
    "    Evaluate the algorithm on all PACS domains.\n",
    "    Returns:\n",
    "      - target_acc: Accuracy on the held-out (target) domain (test split).\n",
    "      - avg_source_acc: Average accuracy over the remaining (source) domains (test splits).\n",
    "    \"\"\"\n",
    "    # Get hyperparameters for PACS – disable augmentation for evaluation.\n",
    "    hparams = hparams_registry.default_hparams(algorithm.__class__.__name__, \"PACS\")\n",
    "    hparams[\"data_augmentation\"] = False  \n",
    "    pacs_dataset = datasets.PACS(root=DATA_DIR, test_envs=[target_env], hparams=hparams)\n",
    "    \n",
    "    accuracies = {}\n",
    "    for env_idx, env_dataset in enumerate(pacs_dataset):\n",
    "        loader = DataLoader(env_dataset, batch_size=hparams[\"batch_size\"], shuffle=False)\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            with torch.no_grad():\n",
    "                # Use the algorithm's predict (or call its network) to get logits.\n",
    "                preds = algorithm.predict(X)\n",
    "                preds = preds.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_samples += len(y)\n",
    "        acc = total_correct / total_samples\n",
    "        accuracies[env_idx] = acc\n",
    "    target_acc = accuracies[target_env]\n",
    "    source_accs = [acc for env, acc in accuracies.items() if env != target_env]\n",
    "    avg_source_acc = sum(source_accs) / len(source_accs)\n",
    "    return target_acc, avg_source_acc\n",
    "\n",
    "def get_sample_batch(algorithm_name, target_domain):\n",
    "    \"\"\"\n",
    "    Get a single mini-batch from the target domain’s test dataset.\n",
    "    This is used for computing gradients for the SNIP pruning method.\n",
    "    \"\"\"\n",
    "    hparams = hparams_registry.default_hparams(algorithm_name, \"PACS\")\n",
    "    hparams[\"data_augmentation\"] = False\n",
    "    pacs_dataset = datasets.PACS(root=DATA_DIR, test_envs=[target_domain], hparams=hparams)\n",
    "    # Note: pacs_dataset is a list of domain datasets; use the target domain.\n",
    "    loader = DataLoader(pacs_dataset[int(target_domain)], batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "    return next(iter(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efff7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_unstructured_pruning(model, prune_level):\n",
    "    \"\"\"\n",
    "    Apply global unstructured L1-based weight magnitude pruning.\n",
    "    Prunes individual weights from Conv2d and Linear layers.\n",
    "    \"\"\"\n",
    "    params_to_prune = []\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            params_to_prune.append((module, 'weight'))\n",
    "    prune.global_unstructured(\n",
    "        params_to_prune, \n",
    "        pruning_method=prune.L1Unstructured, \n",
    "        amount=prune_level\n",
    "    )\n",
    "    for module, param_name in params_to_prune:\n",
    "        prune.remove(module, param_name)\n",
    "    return model\n",
    "\n",
    "def apply_structured_pruning(model, prune_level):\n",
    "    \"\"\"\n",
    "    Apply structured channel pruning on Conv2d layers.\n",
    "    Uses the L1-norm of the filter weights to prune entire output channels.\n",
    "    \"\"\"\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, name='weight', amount=prune_level, n=1, dim=0)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def apply_random_pruning(model, prune_level):\n",
    "    \"\"\"\n",
    "    Apply global unstructured random pruning on Conv2d and Linear layers.\n",
    "    \"\"\"\n",
    "    params_to_prune = []\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            params_to_prune.append((module, 'weight'))\n",
    "    prune.global_unstructured(\n",
    "        params_to_prune, \n",
    "        pruning_method=prune.RandomUnstructured, \n",
    "        amount=prune_level\n",
    "    )\n",
    "    for module, param_name in params_to_prune:\n",
    "        prune.remove(module, param_name)\n",
    "    return model\n",
    "\n",
    "def apply_snip_pruning(model, prune_level, sample_batch):\n",
    "    \"\"\"\n",
    "    Apply a SNIP-inspired pruning method.\n",
    "    Computes saliency scores for each weight as |w * grad| using one mini-batch.\n",
    "    Prunes the bottom prune_level fraction of weights globally.\n",
    "    \n",
    "    sample_batch is a tuple (inputs, labels) from the target domain.\n",
    "    \"\"\"\n",
    "    # Set the model to train mode and zero gradients.\n",
    "    model.train()\n",
    "    inputs, labels = sample_batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Forward pass and compute loss (use cross entropy).\n",
    "    logits = model.predict(inputs)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gather sensitivity scores from eligible layers.\n",
    "    all_scores = []\n",
    "    modules_and_names = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            if module.weight.grad is None:\n",
    "                continue\n",
    "            score = (module.weight * module.weight.grad).abs()\n",
    "            all_scores.append(score.view(-1))\n",
    "            modules_and_names.append((module, 'weight'))\n",
    "    all_scores = torch.cat(all_scores)\n",
    "    \n",
    "    # Determine threshold so that only (1 - prune_level) fraction of weights are kept.\n",
    "    num_params_to_keep = int((1 - prune_level) * all_scores.numel())\n",
    "    if num_params_to_keep < 1:\n",
    "        threshold = all_scores.max() + 1\n",
    "    else:\n",
    "        # kthvalue returns the k-th smallest value.\n",
    "        threshold, _ = torch.kthvalue(all_scores, all_scores.numel() - num_params_to_keep + 1)\n",
    "    \n",
    "    # Apply mask on each eligible parameter.\n",
    "    for (module, param_name) in modules_and_names:\n",
    "        score = (module.weight * module.weight.grad).abs()\n",
    "        mask = (score >= threshold).float()\n",
    "        module.weight.data.mul_(mask)\n",
    "    \n",
    "    # Clear gradients and return to eval mode.\n",
    "    model.zero_grad()\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2daccaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_all_methods(algorithm_name, target_domain, prune_levels):\n",
    "    \"\"\"\n",
    "    For the specified algorithm and target domain, load the pre-trained model,\n",
    "    report baseline performance, and then apply all four pruning methods at each given prune level.\n",
    "    \n",
    "    The performance is evaluated using the test splits of all domains.\n",
    "    Pruned models are saved under:\n",
    "      pruned/{algorithm_name}/{target_domain}/{method}_{prune_percent}/model.pkl\n",
    "    \"\"\"\n",
    "    # Load the original model and dump info.\n",
    "    model_path = os.path.join(\"plain\", algorithm_name, str(target_domain), \"model.pkl\")\n",
    "    model, dump_info = load_algorithm_model(model_path)\n",
    "    \n",
    "    # Report baseline (no pruning) performance.\n",
    "    baseline_target_acc, baseline_source_acc = evaluate_model_on_pacs(model, int(target_domain))\n",
    "    print(f\"Baseline [No Pruning] -> {algorithm_name} Domain: {target_domain} | \"\n",
    "          f\"Target Acc: {baseline_target_acc:.2%}, Source Avg Acc: {baseline_source_acc:.2%}\")\n",
    "    \n",
    "    # Define the pruning methods.\n",
    "    pruning_methods = [\"unstructured\", \"structured\", \"random\", \"snip\"]\n",
    "    \n",
    "    for method in pruning_methods:\n",
    "        for level in prune_levels:\n",
    "            model_to_prune = copy.deepcopy(model)\n",
    "            \n",
    "            if method == \"unstructured\":\n",
    "                model_to_prune = apply_unstructured_pruning(model_to_prune, level)\n",
    "            elif method == \"structured\":\n",
    "                model_to_prune = apply_structured_pruning(model_to_prune, level)\n",
    "            elif method == \"random\":\n",
    "                model_to_prune = apply_random_pruning(model_to_prune, level)\n",
    "            elif method == \"snip\":\n",
    "                # For SNIP, obtain one mini-batch from the target domain.\n",
    "                sample_batch = get_sample_batch(algorithm_name, target_domain)\n",
    "                model_to_prune = apply_snip_pruning(model_to_prune, level, sample_batch)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown pruning method. Choose one of the supported methods.\")\n",
    "            \n",
    "            # Evaluate the pruned model.\n",
    "            target_acc, avg_source_acc = evaluate_model_on_pacs(model_to_prune, int(target_domain))\n",
    "            print(f\"[{method.capitalize()} {int(level*100)}% Pruned] {algorithm_name} Domain: {target_domain} | \"\n",
    "                  f\"Target Acc: {target_acc:.2%}, Source Avg Acc: {avg_source_acc:.2%}\")\n",
    "            \n",
    "            # Save the pruned model.\n",
    "            save_dir = os.path.join(\"pruned\", algorithm_name, str(target_domain), f\"{method}_{int(level*100)}\")\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, \"model.pkl\")\n",
    "            pruned_dump = {\n",
    "                \"args\": dump_info[\"args\"],\n",
    "                \"model_input_shape\": dump_info[\"model_input_shape\"],\n",
    "                \"model_num_classes\": dump_info[\"model_num_classes\"],\n",
    "                \"model_num_domains\": dump_info[\"model_num_domains\"],\n",
    "                \"model_hparams\": dump_info[\"model_hparams\"],\n",
    "                \"model_dict\": model_to_prune.state_dict()\n",
    "            }\n",
    "            torch.save(pruned_dump, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea99fc",
   "metadata": {},
   "source": [
    "### URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "427f7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Target Domain 0 for URM =====\n",
      "--> Initializing discriminator <--\n",
      "--> Modifying encoder output: tanh\n",
      "Baseline [No Pruning] -> URM Domain: 0 | Target Acc: 80.96%, Source Avg Acc: 99.28%\n",
      "[Unstructured 10% Pruned] URM Domain: 0 | Target Acc: 80.91%, Source Avg Acc: 99.27%\n",
      "[Unstructured 30% Pruned] URM Domain: 0 | Target Acc: 80.57%, Source Avg Acc: 99.19%\n",
      "[Unstructured 50% Pruned] URM Domain: 0 | Target Acc: 79.74%, Source Avg Acc: 99.03%\n",
      "[Unstructured 70% Pruned] URM Domain: 0 | Target Acc: 71.34%, Source Avg Acc: 98.20%\n",
      "[Structured 10% Pruned] URM Domain: 0 | Target Acc: 52.15%, Source Avg Acc: 83.10%\n",
      "[Structured 30% Pruned] URM Domain: 0 | Target Acc: 24.12%, Source Avg Acc: 21.15%\n",
      "[Structured 50% Pruned] URM Domain: 0 | Target Acc: 21.92%, Source Avg Acc: 15.74%\n",
      "[Structured 70% Pruned] URM Domain: 0 | Target Acc: 21.92%, Source Avg Acc: 15.74%\n",
      "[Random 10% Pruned] URM Domain: 0 | Target Acc: 22.75%, Source Avg Acc: 26.67%\n",
      "[Random 30% Pruned] URM Domain: 0 | Target Acc: 12.45%, Source Avg Acc: 16.81%\n",
      "[Random 50% Pruned] URM Domain: 0 | Target Acc: 21.92%, Source Avg Acc: 15.74%\n",
      "[Random 70% Pruned] URM Domain: 0 | Target Acc: 12.45%, Source Avg Acc: 16.81%\n",
      "[Snip 10% Pruned] URM Domain: 0 | Target Acc: 81.84%, Source Avg Acc: 99.25%\n",
      "[Snip 30% Pruned] URM Domain: 0 | Target Acc: 82.62%, Source Avg Acc: 99.27%\n",
      "[Snip 50% Pruned] URM Domain: 0 | Target Acc: 73.83%, Source Avg Acc: 98.24%\n",
      "[Snip 70% Pruned] URM Domain: 0 | Target Acc: 12.45%, Source Avg Acc: 16.84%\n",
      "\n",
      "===== Processing Target Domain 1 for URM =====\n",
      "--> Initializing discriminator <--\n",
      "--> Modifying encoder output: tanh\n",
      "Baseline [No Pruning] -> URM Domain: 1 | Target Acc: 77.94%, Source Avg Acc: 99.34%\n",
      "[Unstructured 10% Pruned] URM Domain: 1 | Target Acc: 78.07%, Source Avg Acc: 99.33%\n",
      "[Unstructured 30% Pruned] URM Domain: 1 | Target Acc: 77.43%, Source Avg Acc: 99.39%\n",
      "[Unstructured 50% Pruned] URM Domain: 1 | Target Acc: 80.46%, Source Avg Acc: 99.22%\n",
      "[Unstructured 70% Pruned] URM Domain: 1 | Target Acc: 77.26%, Source Avg Acc: 98.25%\n",
      "[Structured 10% Pruned] URM Domain: 1 | Target Acc: 47.31%, Source Avg Acc: 65.93%\n",
      "[Structured 30% Pruned] URM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Structured 50% Pruned] URM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Structured 70% Pruned] URM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Random 10% Pruned] URM Domain: 1 | Target Acc: 22.31%, Source Avg Acc: 25.09%\n",
      "[Random 30% Pruned] URM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Random 50% Pruned] URM Domain: 1 | Target Acc: 14.76%, Source Avg Acc: 14.66%\n",
      "[Random 70% Pruned] URM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Snip 10% Pruned] URM Domain: 1 | Target Acc: 79.35%, Source Avg Acc: 99.35%\n",
      "[Snip 30% Pruned] URM Domain: 1 | Target Acc: 79.10%, Source Avg Acc: 99.36%\n",
      "[Snip 50% Pruned] URM Domain: 1 | Target Acc: 72.35%, Source Avg Acc: 98.75%\n",
      "[Snip 70% Pruned] URM Domain: 1 | Target Acc: 38.65%, Source Avg Acc: 63.47%\n",
      "\n",
      "===== Processing Target Domain 2 for URM =====\n",
      "--> Initializing discriminator <--\n",
      "--> Modifying encoder output: tanh\n",
      "Baseline [No Pruning] -> URM Domain: 2 | Target Acc: 96.77%, Source Avg Acc: 99.24%\n",
      "[Unstructured 10% Pruned] URM Domain: 2 | Target Acc: 96.77%, Source Avg Acc: 99.24%\n",
      "[Unstructured 30% Pruned] URM Domain: 2 | Target Acc: 96.95%, Source Avg Acc: 99.28%\n",
      "[Unstructured 50% Pruned] URM Domain: 2 | Target Acc: 96.47%, Source Avg Acc: 99.26%\n",
      "[Unstructured 70% Pruned] URM Domain: 2 | Target Acc: 95.63%, Source Avg Acc: 97.58%\n",
      "[Structured 10% Pruned] URM Domain: 2 | Target Acc: 55.45%, Source Avg Acc: 57.57%\n",
      "[Structured 30% Pruned] URM Domain: 2 | Target Acc: 11.32%, Source Avg Acc: 18.25%\n",
      "[Structured 50% Pruned] URM Domain: 2 | Target Acc: 11.32%, Source Avg Acc: 18.25%\n",
      "[Structured 70% Pruned] URM Domain: 2 | Target Acc: 11.92%, Source Avg Acc: 14.80%\n",
      "[Random 10% Pruned] URM Domain: 2 | Target Acc: 41.32%, Source Avg Acc: 27.34%\n",
      "[Random 30% Pruned] URM Domain: 2 | Target Acc: 11.32%, Source Avg Acc: 18.25%\n",
      "[Random 50% Pruned] URM Domain: 2 | Target Acc: 25.87%, Source Avg Acc: 14.42%\n",
      "[Random 70% Pruned] URM Domain: 2 | Target Acc: 11.32%, Source Avg Acc: 18.25%\n",
      "[Snip 10% Pruned] URM Domain: 2 | Target Acc: 97.13%, Source Avg Acc: 99.20%\n",
      "[Snip 30% Pruned] URM Domain: 2 | Target Acc: 96.47%, Source Avg Acc: 99.29%\n",
      "[Snip 50% Pruned] URM Domain: 2 | Target Acc: 95.45%, Source Avg Acc: 97.74%\n",
      "[Snip 70% Pruned] URM Domain: 2 | Target Acc: 15.45%, Source Avg Acc: 19.08%\n",
      "\n",
      "===== Processing Target Domain 3 for URM =====\n",
      "--> Initializing discriminator <--\n",
      "--> Modifying encoder output: tanh\n",
      "Baseline [No Pruning] -> URM Domain: 3 | Target Acc: 75.57%, Source Avg Acc: 99.42%\n",
      "[Unstructured 10% Pruned] URM Domain: 3 | Target Acc: 75.52%, Source Avg Acc: 99.41%\n",
      "[Unstructured 30% Pruned] URM Domain: 3 | Target Acc: 75.67%, Source Avg Acc: 99.34%\n",
      "[Unstructured 50% Pruned] URM Domain: 3 | Target Acc: 76.15%, Source Avg Acc: 99.26%\n",
      "[Unstructured 70% Pruned] URM Domain: 3 | Target Acc: 74.06%, Source Avg Acc: 97.95%\n",
      "[Structured 10% Pruned] URM Domain: 3 | Target Acc: 29.96%, Source Avg Acc: 79.93%\n",
      "[Structured 30% Pruned] URM Domain: 3 | Target Acc: 19.65%, Source Avg Acc: 15.47%\n",
      "[Structured 50% Pruned] URM Domain: 3 | Target Acc: 19.65%, Source Avg Acc: 15.47%\n",
      "[Structured 70% Pruned] URM Domain: 3 | Target Acc: 19.65%, Source Avg Acc: 15.47%\n",
      "[Random 10% Pruned] URM Domain: 3 | Target Acc: 13.64%, Source Avg Acc: 26.23%\n",
      "[Random 30% Pruned] URM Domain: 3 | Target Acc: 4.07%, Source Avg Acc: 21.69%\n",
      "[Random 50% Pruned] URM Domain: 3 | Target Acc: 4.07%, Source Avg Acc: 21.69%\n",
      "[Random 70% Pruned] URM Domain: 3 | Target Acc: 4.07%, Source Avg Acc: 21.69%\n",
      "[Snip 10% Pruned] URM Domain: 3 | Target Acc: 76.38%, Source Avg Acc: 99.34%\n",
      "[Snip 30% Pruned] URM Domain: 3 | Target Acc: 74.88%, Source Avg Acc: 99.34%\n",
      "[Snip 50% Pruned] URM Domain: 3 | Target Acc: 72.26%, Source Avg Acc: 98.81%\n",
      "[Snip 70% Pruned] URM Domain: 3 | Target Acc: 21.48%, Source Avg Acc: 22.64%\n"
     ]
    }
   ],
   "source": [
    "# List all target domains (for PACS, typical target domains are 0, 1, 2, 3)\n",
    "target_domains = [0, 1, 2, 3]\n",
    "\n",
    "# Define your algorithm and prune levels.\n",
    "algorithm_name = \"URM\"            # e.g., \"ERM\", \"IRM\", etc.\n",
    "prune_levels = [0.1, 0.3, 0.5, 0.7]  # Pruning percentages: 10%, 30%, 50%, 70%\n",
    "\n",
    "for t_domain in target_domains:\n",
    "    print(f\"\\n===== Processing Target Domain {t_domain} for {algorithm_name} =====\")\n",
    "    prune_all_methods(algorithm_name, t_domain, prune_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4866273",
   "metadata": {},
   "source": [
    "### EQRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a268abd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Target Domain 0 for EQRM =====\n",
      "Baseline [No Pruning] -> EQRM Domain: 0 | Target Acc: 85.60%, Source Avg Acc: 99.19%\n",
      "[Unstructured 10% Pruned] EQRM Domain: 0 | Target Acc: 85.74%, Source Avg Acc: 99.21%\n",
      "[Unstructured 30% Pruned] EQRM Domain: 0 | Target Acc: 85.40%, Source Avg Acc: 99.17%\n",
      "[Unstructured 50% Pruned] EQRM Domain: 0 | Target Acc: 83.94%, Source Avg Acc: 98.87%\n",
      "[Unstructured 70% Pruned] EQRM Domain: 0 | Target Acc: 64.70%, Source Avg Acc: 95.47%\n",
      "[Structured 10% Pruned] EQRM Domain: 0 | Target Acc: 46.04%, Source Avg Acc: 70.53%\n",
      "[Structured 30% Pruned] EQRM Domain: 0 | Target Acc: 10.25%, Source Avg Acc: 13.98%\n",
      "[Structured 50% Pruned] EQRM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Structured 70% Pruned] EQRM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Random 10% Pruned] EQRM Domain: 0 | Target Acc: 24.02%, Source Avg Acc: 40.46%\n",
      "[Random 30% Pruned] EQRM Domain: 0 | Target Acc: 7.81%, Source Avg Acc: 11.69%\n",
      "[Random 50% Pruned] EQRM Domain: 0 | Target Acc: 9.81%, Source Avg Acc: 15.50%\n",
      "[Random 70% Pruned] EQRM Domain: 0 | Target Acc: 9.81%, Source Avg Acc: 15.50%\n",
      "[Snip 10% Pruned] EQRM Domain: 0 | Target Acc: 86.72%, Source Avg Acc: 99.13%\n",
      "[Snip 30% Pruned] EQRM Domain: 0 | Target Acc: 82.86%, Source Avg Acc: 98.96%\n",
      "[Snip 50% Pruned] EQRM Domain: 0 | Target Acc: 43.95%, Source Avg Acc: 65.95%\n",
      "[Snip 70% Pruned] EQRM Domain: 0 | Target Acc: 14.40%, Source Avg Acc: 10.36%\n",
      "\n",
      "===== Processing Target Domain 1 for EQRM =====\n",
      "Baseline [No Pruning] -> EQRM Domain: 1 | Target Acc: 82.25%, Source Avg Acc: 98.82%\n",
      "[Unstructured 10% Pruned] EQRM Domain: 1 | Target Acc: 81.87%, Source Avg Acc: 98.78%\n",
      "[Unstructured 30% Pruned] EQRM Domain: 1 | Target Acc: 82.81%, Source Avg Acc: 98.85%\n",
      "[Unstructured 50% Pruned] EQRM Domain: 1 | Target Acc: 83.11%, Source Avg Acc: 98.54%\n",
      "[Unstructured 70% Pruned] EQRM Domain: 1 | Target Acc: 79.56%, Source Avg Acc: 95.70%\n",
      "[Structured 10% Pruned] EQRM Domain: 1 | Target Acc: 62.33%, Source Avg Acc: 70.64%\n",
      "[Structured 30% Pruned] EQRM Domain: 1 | Target Acc: 8.70%, Source Avg Acc: 12.37%\n",
      "[Structured 50% Pruned] EQRM Domain: 1 | Target Acc: 5.76%, Source Avg Acc: 11.87%\n",
      "[Structured 70% Pruned] EQRM Domain: 1 | Target Acc: 5.76%, Source Avg Acc: 11.87%\n",
      "[Random 10% Pruned] EQRM Domain: 1 | Target Acc: 38.18%, Source Avg Acc: 30.21%\n",
      "[Random 30% Pruned] EQRM Domain: 1 | Target Acc: 10.03%, Source Avg Acc: 13.01%\n",
      "[Random 50% Pruned] EQRM Domain: 1 | Target Acc: 13.82%, Source Avg Acc: 14.17%\n",
      "[Random 70% Pruned] EQRM Domain: 1 | Target Acc: 14.76%, Source Avg Acc: 14.66%\n",
      "[Snip 10% Pruned] EQRM Domain: 1 | Target Acc: 82.72%, Source Avg Acc: 98.80%\n",
      "[Snip 30% Pruned] EQRM Domain: 1 | Target Acc: 77.90%, Source Avg Acc: 97.92%\n",
      "[Snip 50% Pruned] EQRM Domain: 1 | Target Acc: 38.01%, Source Avg Acc: 61.93%\n",
      "[Snip 70% Pruned] EQRM Domain: 1 | Target Acc: 30.80%, Source Avg Acc: 25.04%\n",
      "\n",
      "===== Processing Target Domain 2 for EQRM =====\n",
      "Baseline [No Pruning] -> EQRM Domain: 2 | Target Acc: 96.89%, Source Avg Acc: 98.67%\n",
      "[Unstructured 10% Pruned] EQRM Domain: 2 | Target Acc: 96.89%, Source Avg Acc: 98.61%\n",
      "[Unstructured 30% Pruned] EQRM Domain: 2 | Target Acc: 97.01%, Source Avg Acc: 98.58%\n",
      "[Unstructured 50% Pruned] EQRM Domain: 2 | Target Acc: 96.11%, Source Avg Acc: 98.14%\n",
      "[Unstructured 70% Pruned] EQRM Domain: 2 | Target Acc: 94.37%, Source Avg Acc: 95.87%\n",
      "[Structured 10% Pruned] EQRM Domain: 2 | Target Acc: 66.65%, Source Avg Acc: 62.98%\n",
      "[Structured 30% Pruned] EQRM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.10%\n",
      "[Structured 50% Pruned] EQRM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.07%\n",
      "[Structured 70% Pruned] EQRM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.07%\n",
      "[Random 10% Pruned] EQRM Domain: 2 | Target Acc: 38.80%, Source Avg Acc: 55.17%\n",
      "[Random 30% Pruned] EQRM Domain: 2 | Target Acc: 26.17%, Source Avg Acc: 14.35%\n",
      "[Random 50% Pruned] EQRM Domain: 2 | Target Acc: 25.87%, Source Avg Acc: 14.36%\n",
      "[Random 70% Pruned] EQRM Domain: 2 | Target Acc: 12.10%, Source Avg Acc: 16.93%\n",
      "[Snip 10% Pruned] EQRM Domain: 2 | Target Acc: 97.13%, Source Avg Acc: 98.78%\n",
      "[Snip 30% Pruned] EQRM Domain: 2 | Target Acc: 85.33%, Source Avg Acc: 80.06%\n",
      "[Snip 50% Pruned] EQRM Domain: 2 | Target Acc: 31.14%, Source Avg Acc: 39.42%\n",
      "[Snip 70% Pruned] EQRM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.07%\n",
      "\n",
      "===== Processing Target Domain 3 for EQRM =====\n",
      "Baseline [No Pruning] -> EQRM Domain: 3 | Target Acc: 84.55%, Source Avg Acc: 99.29%\n",
      "[Unstructured 10% Pruned] EQRM Domain: 3 | Target Acc: 84.55%, Source Avg Acc: 99.31%\n",
      "[Unstructured 30% Pruned] EQRM Domain: 3 | Target Acc: 84.63%, Source Avg Acc: 99.30%\n",
      "[Unstructured 50% Pruned] EQRM Domain: 3 | Target Acc: 85.11%, Source Avg Acc: 99.19%\n",
      "[Unstructured 70% Pruned] EQRM Domain: 3 | Target Acc: 76.20%, Source Avg Acc: 96.33%\n",
      "[Structured 10% Pruned] EQRM Domain: 3 | Target Acc: 50.98%, Source Avg Acc: 79.61%\n",
      "[Structured 30% Pruned] EQRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.83%\n",
      "[Structured 50% Pruned] EQRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Structured 70% Pruned] EQRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Random 10% Pruned] EQRM Domain: 3 | Target Acc: 35.35%, Source Avg Acc: 25.28%\n",
      "[Random 30% Pruned] EQRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 9.61%\n",
      "[Random 50% Pruned] EQRM Domain: 3 | Target Acc: 19.65%, Source Avg Acc: 15.47%\n",
      "[Random 70% Pruned] EQRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Snip 10% Pruned] EQRM Domain: 3 | Target Acc: 83.38%, Source Avg Acc: 99.05%\n",
      "[Snip 30% Pruned] EQRM Domain: 3 | Target Acc: 81.70%, Source Avg Acc: 98.70%\n",
      "[Snip 50% Pruned] EQRM Domain: 3 | Target Acc: 18.83%, Source Avg Acc: 33.29%\n",
      "[Snip 70% Pruned] EQRM Domain: 3 | Target Acc: 29.78%, Source Avg Acc: 31.66%\n"
     ]
    }
   ],
   "source": [
    "# List all target domains (for PACS, typical target domains are 0, 1, 2, 3)\n",
    "target_domains = [0, 1, 2, 3]\n",
    "\n",
    "# Define your algorithm and prune levels.\n",
    "algorithm_name = \"EQRM\"            # e.g., \"ERM\", \"IRM\", etc.\n",
    "prune_levels = [0.1, 0.3, 0.5, 0.7]  # Pruning percentages: 10%, 30%, 50%, 70%\n",
    "\n",
    "for t_domain in target_domains:\n",
    "    print(f\"\\n===== Processing Target Domain {t_domain} for {algorithm_name} =====\")\n",
    "    prune_all_methods(algorithm_name, t_domain, prune_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4c2cc",
   "metadata": {},
   "source": [
    "### ERM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78fd7003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Target Domain 0 for ERMPlusPlus =====\n",
      "Baseline [No Pruning] -> ERMPlusPlus Domain: 0 | Target Acc: 88.62%, Source Avg Acc: 99.64%\n",
      "[Unstructured 10% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 88.48%, Source Avg Acc: 99.64%\n",
      "[Unstructured 30% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 88.33%, Source Avg Acc: 99.65%\n",
      "[Unstructured 50% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 86.38%, Source Avg Acc: 99.63%\n",
      "[Unstructured 70% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 73.19%, Source Avg Acc: 99.15%\n",
      "[Structured 10% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 63.38%, Source Avg Acc: 90.31%\n",
      "[Structured 30% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 15.82%, Source Avg Acc: 15.85%\n",
      "[Structured 50% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 14.84%, Source Avg Acc: 11.16%\n",
      "[Structured 70% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 12.45%, Source Avg Acc: 16.81%\n",
      "[Random 10% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 22.46%, Source Avg Acc: 54.88%\n",
      "[Random 30% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 17.09%, Source Avg Acc: 15.85%\n",
      "[Random 50% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 18.51%, Source Avg Acc: 15.85%\n",
      "[Random 70% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 18.51%, Source Avg Acc: 15.85%\n",
      "[Snip 10% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 87.26%, Source Avg Acc: 99.59%\n",
      "[Snip 30% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 83.40%, Source Avg Acc: 99.07%\n",
      "[Snip 50% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 46.78%, Source Avg Acc: 74.31%\n",
      "[Snip 70% Pruned] ERMPlusPlus Domain: 0 | Target Acc: 23.34%, Source Avg Acc: 26.03%\n",
      "\n",
      "===== Processing Target Domain 1 for ERMPlusPlus =====\n",
      "Baseline [No Pruning] -> ERMPlusPlus Domain: 1 | Target Acc: 83.32%, Source Avg Acc: 99.63%\n",
      "[Unstructured 10% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 83.28%, Source Avg Acc: 99.62%\n",
      "[Unstructured 30% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 83.19%, Source Avg Acc: 99.58%\n",
      "[Unstructured 50% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 85.15%, Source Avg Acc: 99.51%\n",
      "[Unstructured 70% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 82.12%, Source Avg Acc: 99.07%\n",
      "[Structured 10% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 57.34%, Source Avg Acc: 76.10%\n",
      "[Structured 30% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 13.44%, Source Avg Acc: 15.08%\n",
      "[Structured 50% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 14.93%, Source Avg Acc: 12.40%\n",
      "[Structured 70% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 16.60%, Source Avg Acc: 16.49%\n",
      "[Random 10% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 27.05%, Source Avg Acc: 19.66%\n",
      "[Random 30% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 15.70%, Source Avg Acc: 15.10%\n",
      "[Random 50% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 13.82%, Source Avg Acc: 14.17%\n",
      "[Random 70% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 13.82%, Source Avg Acc: 14.17%\n",
      "[Snip 10% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 83.66%, Source Avg Acc: 99.60%\n",
      "[Snip 30% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 84.34%, Source Avg Acc: 99.35%\n",
      "[Snip 50% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 54.22%, Source Avg Acc: 81.04%\n",
      "[Snip 70% Pruned] ERMPlusPlus Domain: 1 | Target Acc: 16.34%, Source Avg Acc: 17.56%\n",
      "\n",
      "===== Processing Target Domain 2 for ERMPlusPlus =====\n",
      "Baseline [No Pruning] -> ERMPlusPlus Domain: 2 | Target Acc: 98.44%, Source Avg Acc: 99.54%\n",
      "[Unstructured 10% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 98.44%, Source Avg Acc: 99.56%\n",
      "[Unstructured 30% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 98.20%, Source Avg Acc: 99.51%\n",
      "[Unstructured 50% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 98.32%, Source Avg Acc: 99.59%\n",
      "[Unstructured 70% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 95.15%, Source Avg Acc: 98.78%\n",
      "[Structured 10% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 76.95%, Source Avg Acc: 79.16%\n",
      "[Structured 30% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 25.93%, Source Avg Acc: 14.17%\n",
      "[Structured 50% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 6.41%, Source Avg Acc: 9.92%\n",
      "[Structured 70% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 12.10%, Source Avg Acc: 16.93%\n",
      "[Random 10% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 18.26%, Source Avg Acc: 38.80%\n",
      "[Random 30% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 10.96%, Source Avg Acc: 14.15%\n",
      "[Random 50% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 25.87%, Source Avg Acc: 14.42%\n",
      "[Random 70% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 11.32%, Source Avg Acc: 18.25%\n",
      "[Snip 10% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 98.44%, Source Avg Acc: 99.40%\n",
      "[Snip 30% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 94.19%, Source Avg Acc: 91.61%\n",
      "[Snip 50% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 73.47%, Source Avg Acc: 52.92%\n",
      "[Snip 70% Pruned] ERMPlusPlus Domain: 2 | Target Acc: 12.10%, Source Avg Acc: 16.93%\n",
      "\n",
      "===== Processing Target Domain 3 for ERMPlusPlus =====\n",
      "Baseline [No Pruning] -> ERMPlusPlus Domain: 3 | Target Acc: 82.85%, Source Avg Acc: 99.65%\n",
      "[Unstructured 10% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 82.82%, Source Avg Acc: 99.67%\n",
      "[Unstructured 30% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 84.04%, Source Avg Acc: 99.65%\n",
      "[Unstructured 50% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 82.79%, Source Avg Acc: 99.58%\n",
      "[Unstructured 70% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 76.23%, Source Avg Acc: 98.82%\n",
      "[Structured 10% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 57.44%, Source Avg Acc: 88.76%\n",
      "[Structured 30% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 4.07%, Source Avg Acc: 21.64%\n",
      "[Structured 50% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 2.04%, Source Avg Acc: 14.50%\n",
      "[Structured 70% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 18.83%, Source Avg Acc: 14.68%\n",
      "[Random 10% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 27.26%, Source Avg Acc: 16.05%\n",
      "[Random 30% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 19.17%, Source Avg Acc: 13.19%\n",
      "[Random 50% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 20.77%, Source Avg Acc: 11.85%\n",
      "[Random 70% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Snip 10% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 82.74%, Source Avg Acc: 99.73%\n",
      "[Snip 30% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 80.30%, Source Avg Acc: 99.54%\n",
      "[Snip 50% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 60.42%, Source Avg Acc: 75.59%\n",
      "[Snip 70% Pruned] ERMPlusPlus Domain: 3 | Target Acc: 19.17%, Source Avg Acc: 13.40%\n"
     ]
    }
   ],
   "source": [
    "# List all target domains (for PACS, typical target domains are 0, 1, 2, 3)\n",
    "target_domains = [0, 1, 2, 3]\n",
    "\n",
    "# Define your algorithm and prune levels.\n",
    "algorithm_name = \"ERMPlusPlus\"            # e.g., \"ERM\", \"IRM\", etc.\n",
    "prune_levels = [0.1, 0.3, 0.5, 0.7]  # Pruning percentages: 10%, 30%, 50%, 70%\n",
    "\n",
    "for t_domain in target_domains:\n",
    "    print(f\"\\n===== Processing Target Domain {t_domain} for {algorithm_name} =====\")\n",
    "    prune_all_methods(algorithm_name, t_domain, prune_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df4417",
   "metadata": {},
   "source": [
    "### IRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75a800b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Target Domain 0 for IRM =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh/miniconda3/envs/octcpll/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rishabh/miniconda3/envs/octcpll/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline [No Pruning] -> IRM Domain: 0 | Target Acc: 32.42%, Source Avg Acc: 61.85%\n",
      "[Unstructured 10% Pruned] IRM Domain: 0 | Target Acc: 32.28%, Source Avg Acc: 61.96%\n",
      "[Unstructured 30% Pruned] IRM Domain: 0 | Target Acc: 33.06%, Source Avg Acc: 62.34%\n",
      "[Unstructured 50% Pruned] IRM Domain: 0 | Target Acc: 33.30%, Source Avg Acc: 63.60%\n",
      "[Unstructured 70% Pruned] IRM Domain: 0 | Target Acc: 19.63%, Source Avg Acc: 35.68%\n",
      "[Structured 10% Pruned] IRM Domain: 0 | Target Acc: 9.81%, Source Avg Acc: 13.21%\n",
      "[Structured 30% Pruned] IRM Domain: 0 | Target Acc: 14.40%, Source Avg Acc: 10.36%\n",
      "[Structured 50% Pruned] IRM Domain: 0 | Target Acc: 14.40%, Source Avg Acc: 10.36%\n",
      "[Structured 70% Pruned] IRM Domain: 0 | Target Acc: 14.40%, Source Avg Acc: 10.36%\n",
      "[Random 10% Pruned] IRM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Random 30% Pruned] IRM Domain: 0 | Target Acc: 10.89%, Source Avg Acc: 11.50%\n",
      "[Random 50% Pruned] IRM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Random 70% Pruned] IRM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Snip 10% Pruned] IRM Domain: 0 | Target Acc: 34.96%, Source Avg Acc: 62.82%\n",
      "[Snip 30% Pruned] IRM Domain: 0 | Target Acc: 34.91%, Source Avg Acc: 62.88%\n",
      "[Snip 50% Pruned] IRM Domain: 0 | Target Acc: 34.47%, Source Avg Acc: 63.71%\n",
      "[Snip 70% Pruned] IRM Domain: 0 | Target Acc: 27.54%, Source Avg Acc: 38.25%\n",
      "\n",
      "===== Processing Target Domain 1 for IRM =====\n",
      "Baseline [No Pruning] -> IRM Domain: 1 | Target Acc: 41.51%, Source Avg Acc: 70.55%\n",
      "[Unstructured 10% Pruned] IRM Domain: 1 | Target Acc: 41.72%, Source Avg Acc: 70.47%\n",
      "[Unstructured 30% Pruned] IRM Domain: 1 | Target Acc: 42.79%, Source Avg Acc: 71.55%\n",
      "[Unstructured 50% Pruned] IRM Domain: 1 | Target Acc: 51.71%, Source Avg Acc: 75.54%\n",
      "[Unstructured 70% Pruned] IRM Domain: 1 | Target Acc: 62.29%, Source Avg Acc: 71.60%\n",
      "[Structured 10% Pruned] IRM Domain: 1 | Target Acc: 30.46%, Source Avg Acc: 31.38%\n",
      "[Structured 30% Pruned] IRM Domain: 1 | Target Acc: 17.32%, Source Avg Acc: 17.13%\n",
      "[Structured 50% Pruned] IRM Domain: 1 | Target Acc: 7.89%, Source Avg Acc: 21.86%\n",
      "[Structured 70% Pruned] IRM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Random 10% Pruned] IRM Domain: 1 | Target Acc: 20.69%, Source Avg Acc: 29.24%\n",
      "[Random 30% Pruned] IRM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Random 50% Pruned] IRM Domain: 1 | Target Acc: 12.24%, Source Avg Acc: 10.97%\n",
      "[Random 70% Pruned] IRM Domain: 1 | Target Acc: 17.28%, Source Avg Acc: 17.29%\n",
      "[Snip 10% Pruned] IRM Domain: 1 | Target Acc: 44.16%, Source Avg Acc: 70.93%\n",
      "[Snip 30% Pruned] IRM Domain: 1 | Target Acc: 43.86%, Source Avg Acc: 70.47%\n",
      "[Snip 50% Pruned] IRM Domain: 1 | Target Acc: 48.21%, Source Avg Acc: 71.66%\n",
      "[Snip 70% Pruned] IRM Domain: 1 | Target Acc: 52.82%, Source Avg Acc: 66.53%\n",
      "\n",
      "===== Processing Target Domain 2 for IRM =====\n",
      "Baseline [No Pruning] -> IRM Domain: 2 | Target Acc: 65.75%, Source Avg Acc: 67.47%\n",
      "[Unstructured 10% Pruned] IRM Domain: 2 | Target Acc: 65.93%, Source Avg Acc: 67.35%\n",
      "[Unstructured 30% Pruned] IRM Domain: 2 | Target Acc: 65.93%, Source Avg Acc: 67.58%\n",
      "[Unstructured 50% Pruned] IRM Domain: 2 | Target Acc: 65.75%, Source Avg Acc: 69.42%\n",
      "[Unstructured 70% Pruned] IRM Domain: 2 | Target Acc: 66.29%, Source Avg Acc: 70.44%\n",
      "[Structured 10% Pruned] IRM Domain: 2 | Target Acc: 42.10%, Source Avg Acc: 31.53%\n",
      "[Structured 30% Pruned] IRM Domain: 2 | Target Acc: 27.01%, Source Avg Acc: 13.76%\n",
      "[Structured 50% Pruned] IRM Domain: 2 | Target Acc: 11.08%, Source Avg Acc: 17.48%\n",
      "[Structured 70% Pruned] IRM Domain: 2 | Target Acc: 11.32%, Source Avg Acc: 18.25%\n",
      "[Random 10% Pruned] IRM Domain: 2 | Target Acc: 30.30%, Source Avg Acc: 19.73%\n",
      "[Random 30% Pruned] IRM Domain: 2 | Target Acc: 11.68%, Source Avg Acc: 17.27%\n",
      "[Random 50% Pruned] IRM Domain: 2 | Target Acc: 16.77%, Source Avg Acc: 9.58%\n",
      "[Random 70% Pruned] IRM Domain: 2 | Target Acc: 16.77%, Source Avg Acc: 9.58%\n",
      "[Snip 10% Pruned] IRM Domain: 2 | Target Acc: 67.54%, Source Avg Acc: 67.50%\n",
      "[Snip 30% Pruned] IRM Domain: 2 | Target Acc: 66.41%, Source Avg Acc: 67.65%\n",
      "[Snip 50% Pruned] IRM Domain: 2 | Target Acc: 65.45%, Source Avg Acc: 68.11%\n",
      "[Snip 70% Pruned] IRM Domain: 2 | Target Acc: 52.22%, Source Avg Acc: 53.62%\n",
      "\n",
      "===== Processing Target Domain 3 for IRM =====\n",
      "Baseline [No Pruning] -> IRM Domain: 3 | Target Acc: 17.23%, Source Avg Acc: 76.52%\n",
      "[Unstructured 10% Pruned] IRM Domain: 3 | Target Acc: 17.28%, Source Avg Acc: 76.50%\n",
      "[Unstructured 30% Pruned] IRM Domain: 3 | Target Acc: 17.15%, Source Avg Acc: 77.62%\n",
      "[Unstructured 50% Pruned] IRM Domain: 3 | Target Acc: 30.75%, Source Avg Acc: 78.47%\n",
      "[Unstructured 70% Pruned] IRM Domain: 3 | Target Acc: 40.11%, Source Avg Acc: 65.16%\n",
      "[Structured 10% Pruned] IRM Domain: 3 | Target Acc: 13.97%, Source Avg Acc: 18.24%\n",
      "[Structured 30% Pruned] IRM Domain: 3 | Target Acc: 2.04%, Source Avg Acc: 15.09%\n",
      "[Structured 50% Pruned] IRM Domain: 3 | Target Acc: 2.04%, Source Avg Acc: 14.49%\n",
      "[Structured 70% Pruned] IRM Domain: 3 | Target Acc: 2.04%, Source Avg Acc: 14.49%\n",
      "[Random 10% Pruned] IRM Domain: 3 | Target Acc: 18.89%, Source Avg Acc: 23.99%\n",
      "[Random 30% Pruned] IRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Random 50% Pruned] IRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Random 70% Pruned] IRM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Snip 10% Pruned] IRM Domain: 3 | Target Acc: 13.51%, Source Avg Acc: 67.62%\n",
      "[Snip 30% Pruned] IRM Domain: 3 | Target Acc: 19.29%, Source Avg Acc: 76.22%\n",
      "[Snip 50% Pruned] IRM Domain: 3 | Target Acc: 14.13%, Source Avg Acc: 60.39%\n",
      "[Snip 70% Pruned] IRM Domain: 3 | Target Acc: 7.89%, Source Avg Acc: 30.91%\n"
     ]
    }
   ],
   "source": [
    "# List all target domains (for PACS, typical target domains are 0, 1, 2, 3)\n",
    "target_domains = [0, 1, 2, 3]\n",
    "\n",
    "# Define your algorithm and prune levels.\n",
    "algorithm_name = \"IRM\"            # e.g., \"ERM\", \"IRM\", etc.\n",
    "prune_levels = [0.1, 0.3, 0.5, 0.7]  # Pruning percentages: 10%, 30%, 50%, 70%\n",
    "\n",
    "for t_domain in target_domains:\n",
    "    print(f\"\\n===== Processing Target Domain {t_domain} for {algorithm_name} =====\")\n",
    "    prune_all_methods(algorithm_name, t_domain, prune_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d0aae",
   "metadata": {},
   "source": [
    "### ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32be3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Target Domain 0 for ERM =====\n",
      "Baseline [No Pruning] -> ERM Domain: 0 | Target Acc: 84.52%, Source Avg Acc: 99.33%\n",
      "[Unstructured 10% Pruned] ERM Domain: 0 | Target Acc: 84.47%, Source Avg Acc: 99.33%\n",
      "[Unstructured 30% Pruned] ERM Domain: 0 | Target Acc: 84.33%, Source Avg Acc: 99.36%\n",
      "[Unstructured 50% Pruned] ERM Domain: 0 | Target Acc: 82.71%, Source Avg Acc: 99.24%\n",
      "[Unstructured 70% Pruned] ERM Domain: 0 | Target Acc: 68.80%, Source Avg Acc: 97.75%\n",
      "[Structured 10% Pruned] ERM Domain: 0 | Target Acc: 45.41%, Source Avg Acc: 87.66%\n",
      "[Structured 30% Pruned] ERM Domain: 0 | Target Acc: 17.43%, Source Avg Acc: 15.55%\n",
      "[Structured 50% Pruned] ERM Domain: 0 | Target Acc: 8.69%, Source Avg Acc: 11.37%\n",
      "[Structured 70% Pruned] ERM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Random 10% Pruned] ERM Domain: 0 | Target Acc: 18.46%, Source Avg Acc: 44.83%\n",
      "[Random 30% Pruned] ERM Domain: 0 | Target Acc: 10.89%, Source Avg Acc: 12.10%\n",
      "[Random 50% Pruned] ERM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Random 70% Pruned] ERM Domain: 0 | Target Acc: 8.98%, Source Avg Acc: 10.79%\n",
      "[Snip 10% Pruned] ERM Domain: 0 | Target Acc: 84.86%, Source Avg Acc: 99.33%\n",
      "[Snip 30% Pruned] ERM Domain: 0 | Target Acc: 83.35%, Source Avg Acc: 99.37%\n",
      "[Snip 50% Pruned] ERM Domain: 0 | Target Acc: 39.60%, Source Avg Acc: 68.80%\n",
      "[Snip 70% Pruned] ERM Domain: 0 | Target Acc: 20.61%, Source Avg Acc: 21.86%\n",
      "\n",
      "===== Processing Target Domain 1 for ERM =====\n",
      "Baseline [No Pruning] -> ERM Domain: 1 | Target Acc: 78.92%, Source Avg Acc: 99.37%\n",
      "[Unstructured 10% Pruned] ERM Domain: 1 | Target Acc: 78.71%, Source Avg Acc: 99.37%\n",
      "[Unstructured 30% Pruned] ERM Domain: 1 | Target Acc: 79.56%, Source Avg Acc: 99.38%\n",
      "[Unstructured 50% Pruned] ERM Domain: 1 | Target Acc: 80.93%, Source Avg Acc: 99.30%\n",
      "[Unstructured 70% Pruned] ERM Domain: 1 | Target Acc: 80.72%, Source Avg Acc: 98.81%\n",
      "[Structured 10% Pruned] ERM Domain: 1 | Target Acc: 61.01%, Source Avg Acc: 80.98%\n",
      "[Structured 30% Pruned] ERM Domain: 1 | Target Acc: 7.68%, Source Avg Acc: 11.26%\n",
      "[Structured 50% Pruned] ERM Domain: 1 | Target Acc: 5.80%, Source Avg Acc: 11.93%\n",
      "[Structured 70% Pruned] ERM Domain: 1 | Target Acc: 6.02%, Source Avg Acc: 12.64%\n",
      "[Random 10% Pruned] ERM Domain: 1 | Target Acc: 50.51%, Source Avg Acc: 38.75%\n",
      "[Random 30% Pruned] ERM Domain: 1 | Target Acc: 13.82%, Source Avg Acc: 14.24%\n",
      "[Random 50% Pruned] ERM Domain: 1 | Target Acc: 13.82%, Source Avg Acc: 14.17%\n",
      "[Random 70% Pruned] ERM Domain: 1 | Target Acc: 14.76%, Source Avg Acc: 14.66%\n",
      "[Snip 10% Pruned] ERM Domain: 1 | Target Acc: 79.99%, Source Avg Acc: 99.37%\n",
      "[Snip 30% Pruned] ERM Domain: 1 | Target Acc: 79.05%, Source Avg Acc: 99.36%\n",
      "[Snip 50% Pruned] ERM Domain: 1 | Target Acc: 62.67%, Source Avg Acc: 94.31%\n",
      "[Snip 70% Pruned] ERM Domain: 1 | Target Acc: 15.10%, Source Avg Acc: 15.14%\n",
      "\n",
      "===== Processing Target Domain 2 for ERM =====\n",
      "Baseline [No Pruning] -> ERM Domain: 2 | Target Acc: 96.65%, Source Avg Acc: 99.24%\n",
      "[Unstructured 10% Pruned] ERM Domain: 2 | Target Acc: 96.65%, Source Avg Acc: 99.22%\n",
      "[Unstructured 30% Pruned] ERM Domain: 2 | Target Acc: 96.17%, Source Avg Acc: 99.12%\n",
      "[Unstructured 50% Pruned] ERM Domain: 2 | Target Acc: 95.99%, Source Avg Acc: 99.05%\n",
      "[Unstructured 70% Pruned] ERM Domain: 2 | Target Acc: 91.92%, Source Avg Acc: 97.61%\n",
      "[Structured 10% Pruned] ERM Domain: 2 | Target Acc: 70.12%, Source Avg Acc: 76.15%\n",
      "[Structured 30% Pruned] ERM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.07%\n",
      "[Structured 50% Pruned] ERM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.07%\n",
      "[Structured 70% Pruned] ERM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.07%\n",
      "[Random 10% Pruned] ERM Domain: 2 | Target Acc: 31.32%, Source Avg Acc: 53.99%\n",
      "[Random 30% Pruned] ERM Domain: 2 | Target Acc: 11.92%, Source Avg Acc: 14.80%\n",
      "[Random 50% Pruned] ERM Domain: 2 | Target Acc: 11.14%, Source Avg Acc: 10.06%\n",
      "[Random 70% Pruned] ERM Domain: 2 | Target Acc: 25.87%, Source Avg Acc: 14.42%\n",
      "[Snip 10% Pruned] ERM Domain: 2 | Target Acc: 96.71%, Source Avg Acc: 99.17%\n",
      "[Snip 30% Pruned] ERM Domain: 2 | Target Acc: 82.51%, Source Avg Acc: 92.32%\n",
      "[Snip 50% Pruned] ERM Domain: 2 | Target Acc: 41.20%, Source Avg Acc: 36.41%\n",
      "[Snip 70% Pruned] ERM Domain: 2 | Target Acc: 12.57%, Source Avg Acc: 12.18%\n",
      "\n",
      "===== Processing Target Domain 3 for ERM =====\n",
      "Baseline [No Pruning] -> ERM Domain: 3 | Target Acc: 76.46%, Source Avg Acc: 99.23%\n",
      "[Unstructured 10% Pruned] ERM Domain: 3 | Target Acc: 76.38%, Source Avg Acc: 99.23%\n",
      "[Unstructured 30% Pruned] ERM Domain: 3 | Target Acc: 76.08%, Source Avg Acc: 99.28%\n",
      "[Unstructured 50% Pruned] ERM Domain: 3 | Target Acc: 75.95%, Source Avg Acc: 99.17%\n",
      "[Unstructured 70% Pruned] ERM Domain: 3 | Target Acc: 69.43%, Source Avg Acc: 98.08%\n",
      "[Structured 10% Pruned] ERM Domain: 3 | Target Acc: 52.74%, Source Avg Acc: 88.71%\n",
      "[Structured 30% Pruned] ERM Domain: 3 | Target Acc: 7.79%, Source Avg Acc: 14.74%\n",
      "[Structured 50% Pruned] ERM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Structured 70% Pruned] ERM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Random 10% Pruned] ERM Domain: 3 | Target Acc: 44.34%, Source Avg Acc: 24.05%\n",
      "[Random 30% Pruned] ERM Domain: 3 | Target Acc: 10.08%, Source Avg Acc: 18.99%\n",
      "[Random 50% Pruned] ERM Domain: 3 | Target Acc: 20.77%, Source Avg Acc: 11.85%\n",
      "[Random 70% Pruned] ERM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n",
      "[Snip 10% Pruned] ERM Domain: 3 | Target Acc: 75.54%, Source Avg Acc: 99.07%\n",
      "[Snip 30% Pruned] ERM Domain: 3 | Target Acc: 77.86%, Source Avg Acc: 99.03%\n",
      "[Snip 50% Pruned] ERM Domain: 3 | Target Acc: 41.54%, Source Avg Acc: 74.40%\n",
      "[Snip 70% Pruned] ERM Domain: 3 | Target Acc: 15.47%, Source Avg Acc: 8.63%\n"
     ]
    }
   ],
   "source": [
    "# List all target domains (for PACS, typical target domains are 0, 1, 2, 3)\n",
    "target_domains = [0, 1, 2, 3]\n",
    "\n",
    "# Define your algorithm and prune levels.\n",
    "algorithm_name = \"ERM\"            # e.g., \"ERM\", \"IRM\", etc.\n",
    "prune_levels = [0.1, 0.3, 0.5, 0.7]  # Pruning percentages: 10%, 30%, 50%, 70%\n",
    "\n",
    "for t_domain in target_domains:\n",
    "    print(f\"\\n===== Processing Target Domain {t_domain} for {algorithm_name} =====\")\n",
    "    prune_all_methods(algorithm_name, t_domain, prune_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a82c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh/miniconda3/envs/octcpll/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rishabh/miniconda3/envs/octcpll/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          method  prune_level  total_params  nonzero_params  model_size_MB\n",
      "0   unstructured          0.1      23522375        21175450       89.73074\n",
      "1   unstructured          0.5      23522375        11787751       89.73074\n",
      "2   unstructured          0.7      23522375         7093901       89.73074\n",
      "3   unstructured          0.3      23522375        16481601       89.73074\n",
      "4     structured          0.7      23522375         7111280       89.73074\n",
      "5     structured          0.1      23522375        21172949       89.73074\n",
      "6     structured          0.5      23522375        11794919       89.73074\n",
      "7     structured          0.3      23522375        16478558       89.73074\n",
      "8         random          0.3      23522375        16481601       89.73074\n",
      "9         random          0.1      23522375        21175450       89.73074\n",
      "10        random          0.5      23522375        11787751       89.73074\n",
      "11        random          0.7      23522375         7093901       89.73074\n"
     ]
    }
   ],
   "source": [
    "# import glob\n",
    "# import torch\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def model_stats(model):\n",
    "#     \"\"\"\n",
    "#     Compute the total number of parameters, number of nonzero parameters,\n",
    "#     and the estimated size (in bytes) of the model.\n",
    "#     \"\"\"\n",
    "#     total_params = sum(p.numel() for p in model.parameters())\n",
    "#     nonzero_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "#     size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "#     return total_params, nonzero_params, size_bytes\n",
    "\n",
    "# # Specify your algorithm name and target domain (as used in folder names)\n",
    "# algorithm_name = \"ERM\"  # Change if needed\n",
    "# target_domain = \"0\"     # Folder name as string\n",
    "\n",
    "# # Root directory where pruned models are stored.\n",
    "# pruned_root = os.path.join(\"pruned\", algorithm_name, target_domain)\n",
    "\n",
    "# # Define the pruning methods you have\n",
    "# pruning_methods = [\"unstructured\", \"structured\", \"random\"]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Loop over methods and all prune levels for each method\n",
    "# for method in pruning_methods:\n",
    "#     # The pattern will match any directory like 'unstructured_10', 'structured_30', etc.\n",
    "#     pattern = os.path.join(pruned_root, f\"{method}_*\", \"model.pkl\")\n",
    "#     for model_path in glob.glob(pattern):\n",
    "#         # Extract pruning level percentage from directory name.\n",
    "#         # E.g., if model_path is pruned/ERM/0/unstructured_10/model.pkl, then '10' is extracted.\n",
    "#         parts = model_path.split(os.path.sep)\n",
    "#         pruning_dir = parts[-2]           # e.g., \"unstructured_10\"\n",
    "#         prune_level_str = pruning_dir.split(\"_\")[1]  # \"10\"\n",
    "#         prune_level = int(prune_level_str) / 100.0   # Convert to fraction (e.g., 0.1)\n",
    "\n",
    "#         # Load the pruned model\n",
    "#         model, _ = load_algorithm_model(model_path)\n",
    "#         total_params, nonzero_params, size_bytes = model_stats(model)\n",
    "\n",
    "#         results.append({\n",
    "#             \"method\": method,\n",
    "#             \"prune_level\": prune_level,\n",
    "#             \"total_params\": total_params,\n",
    "#             \"nonzero_params\": nonzero_params,\n",
    "#             \"model_size_MB\": size_bytes / (1024 ** 2)\n",
    "#         })\n",
    "\n",
    "# # Create and display a summary table using pandas DataFrame\n",
    "# df = pd.DataFrame(results)\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29f494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octcpll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
